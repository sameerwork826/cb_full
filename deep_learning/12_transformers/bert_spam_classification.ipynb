{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581d125f",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Codebasics DL Course: SPAM Classification Using BERT<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbec7d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87905eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115d266",
   "metadata": {},
   "source": [
    "### Dataset Credits\n",
    "\n",
    "This dataset is taken from UCI Machine Learning Respository: https://archive.ics.uci.edu/dataset/228/sms+spam+collection\n",
    "\n",
    "Refer to dataset_credits_readme.txt file for credits details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e1a43b",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80401336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b57196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category = df.Category.map({'ham':0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7cbe47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    4825\n",
       "1     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e108f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam = df[df.Category==1]\n",
    "df_spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c54e776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ham_small = df[df.Category==0].sample(1000)\n",
    "df_ham_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8073028c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    1000\n",
       "1     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = pd.concat([df_spam, df_ham_small])\n",
    "df_small.Category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569a089",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b8b185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_small.Message,df_small.Category, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05cbf373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3642    1\n",
       "494     0\n",
       "2458    0\n",
       "19      1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48bad4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    789\n",
       "1    608\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacb5501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "0    211\n",
       "1    139\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810b028",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "548157d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  9241,  2039,  1010, 11562,  2182,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1045,  2097,  2156,  2017,  4826,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([1., 0.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(texts, labels):\n",
    "    encodings = tokenizer(texts, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "    return encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "tokenize_function([\"Hurry up, click here\", \"I will see you tomorrow\"], [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01b094b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You can stop further club tones by replying \"STOP MIX\" See my-tone.com/enjoy. html for terms. Club tones cost GBP4.50/week. MFL, PO Box 1146 MK45 2WT (2/3)',\n",
       " 'What class of  &lt;#&gt;  reunion?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values.tolist()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd5ac64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3642    1\n",
       "494     0\n",
       "2458    0\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5f2d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_attention_mask, train_labels = tokenize_function(X_train.values.tolist(), y_train.values.tolist())\n",
    "val_input_ids, val_attention_mask, val_labels = tokenize_function(X_test.values.tolist(), y_test.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a0eb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_input_ids, val_attention_mask, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "957816dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c29c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f4dce",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f950898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False # Freeze all BERT layers\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sentence_embedding = bert_output.last_hidden_state[:,0,:]\n",
    "        return self.classifier(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82c73d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c38852",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6048a98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Epoch: 0, Loss:  0.69\n",
      "Batch: 1, Epoch: 0, Loss:  0.66\n",
      "Batch: 2, Epoch: 0, Loss:  0.60\n",
      "Batch: 3, Epoch: 0, Loss:  0.54\n",
      "Batch: 4, Epoch: 0, Loss:  0.47\n",
      "Batch: 5, Epoch: 0, Loss:  0.43\n",
      "Batch: 6, Epoch: 0, Loss:  0.43\n",
      "Batch: 7, Epoch: 0, Loss:  0.32\n",
      "Batch: 8, Epoch: 0, Loss:  0.32\n",
      "Batch: 9, Epoch: 0, Loss:  0.27\n",
      "Batch: 10, Epoch: 0, Loss:  0.29\n",
      "Batch: 11, Epoch: 0, Loss:  0.20\n",
      "Batch: 12, Epoch: 0, Loss:  0.17\n",
      "Batch: 13, Epoch: 0, Loss:  0.14\n",
      "Batch: 14, Epoch: 0, Loss:  0.21\n",
      "Batch: 15, Epoch: 0, Loss:  0.14\n",
      "Batch: 16, Epoch: 0, Loss:  0.18\n",
      "Batch: 17, Epoch: 0, Loss:  0.16\n",
      "Batch: 18, Epoch: 0, Loss:  0.19\n",
      "Batch: 19, Epoch: 0, Loss:  0.08\n",
      "Batch: 20, Epoch: 0, Loss:  0.11\n",
      "Batch: 21, Epoch: 0, Loss:  0.06\n",
      "Epoch 1/2, Training Loss: 0.3022423481399363\n",
      "Batch: 0, Epoch: 1, Loss:  0.10\n",
      "Batch: 1, Epoch: 1, Loss:  0.24\n",
      "Batch: 2, Epoch: 1, Loss:  0.06\n",
      "Batch: 3, Epoch: 1, Loss:  0.11\n",
      "Batch: 4, Epoch: 1, Loss:  0.12\n",
      "Batch: 5, Epoch: 1, Loss:  0.21\n",
      "Batch: 6, Epoch: 1, Loss:  0.11\n",
      "Batch: 7, Epoch: 1, Loss:  0.13\n",
      "Batch: 8, Epoch: 1, Loss:  0.04\n",
      "Batch: 9, Epoch: 1, Loss:  0.06\n",
      "Batch: 10, Epoch: 1, Loss:  0.11\n",
      "Batch: 11, Epoch: 1, Loss:  0.09\n",
      "Batch: 12, Epoch: 1, Loss:  0.20\n",
      "Batch: 13, Epoch: 1, Loss:  0.12\n",
      "Batch: 14, Epoch: 1, Loss:  0.24\n",
      "Batch: 15, Epoch: 1, Loss:  0.12\n",
      "Batch: 16, Epoch: 1, Loss:  0.10\n",
      "Batch: 17, Epoch: 1, Loss:  0.05\n",
      "Batch: 18, Epoch: 1, Loss:  0.14\n",
      "Batch: 19, Epoch: 1, Loss:  0.09\n",
      "Batch: 20, Epoch: 1, Loss:  0.03\n",
      "Batch: 21, Epoch: 1, Loss:  0.05\n",
      "Epoch 2/2, Training Loss: 0.11421464028006251\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Batch: {batch}, Epoch: {epoch}, Loss:  {loss.item():0.2f}\")\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b0f13",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8050ec51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.08298120034548144, Validation Accuracy: 0.9743\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in val_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask).squeeze()\n",
    "        loss = criterion(outputs, labels.view_as(outputs))\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        preds = (outputs > 0.5).float()\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        \n",
    "avg_val_loss = total_val_loss / len(val_loader)\n",
    "val_accuracy = correct_predictions.double() / len(val_dataset)\n",
    "print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy:.4f}')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f3929",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "743fae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, max_length=128):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Tokenize input text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask).squeeze()\n",
    "        prediction = (output > 0.5).float().item()\n",
    "        return 'spam' if prediction == 1 else 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2e5678e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"This is your last chance to WIN cash. Click this link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e9e2019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"Dear Dhaval, I hope to see you on Monday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1525974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
