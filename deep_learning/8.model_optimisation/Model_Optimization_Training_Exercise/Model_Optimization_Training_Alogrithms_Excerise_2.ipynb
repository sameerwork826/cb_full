{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpRkAlxQwQjy"
      },
      "source": [
        "# Problem Statement: **BONUS EXERCISE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvpy57daLojm"
      },
      "source": [
        "Imports and CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hOiJAjpnklhJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYMNU-bmZXl"
      },
      "source": [
        "Problem 1: **Gradient Descent for Demand Forecasting at AtliQ**\n",
        "\n",
        "AtliQ wants to optimize the prediction of regional product demands using gradient descent.\n",
        "\n",
        "Assume the loss function is\n",
        "\n",
        "$$L(w)=(w‚àí4)^2$$\n",
        "\n",
        "where **w** is a weight parameter initialized at 0.\n",
        "\n",
        "**Write code to:**\n",
        "\n",
        "* Perform 10 iterations of gradient descent using a learning rate of 0.1.\n",
        "\n",
        "* Print the weight **w** at each step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bhYyNy9mndvd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: w = 0.8000\n",
            "Step 2: w = 1.4400\n",
            "Step 3: w = 1.9520\n",
            "Step 4: w = 2.3616\n",
            "Step 5: w = 2.6893\n",
            "Step 6: w = 2.9514\n",
            "Step 7: w = 3.1611\n",
            "Step 8: w = 3.3289\n",
            "Step 9: w = 3.4631\n",
            "Step 10: w = 3.5705\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.1\n",
        "w = 0.0\n",
        "\n",
        "for i in range (10):\n",
        "  gradient = 2*(w-4)\n",
        "  w =w-learning_rate*gradient\n",
        "  print(f\"Step {i+1}: w = {w:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xADdX0ULpDhP"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40THnZYtoOkV"
      },
      "source": [
        "Problem 2: **Momentum for Contour Navigation in AtliQ's Supply Chain**\n",
        "\n",
        "AtliQ's supply chain optimization problem is represented by a contour map of a quadratic function:\n",
        "\n",
        "$$f(x,y)=x^2 +3y^2$$\n",
        "\n",
        "Write a code to implement gradient descent (5 iterations) with momentum to minimize this function.\n",
        "\n",
        "Use:\n",
        "* Initial point (x, y) = (2, 2)\n",
        "* Learning rate (Œ∑) = 0.1\n",
        "* Momentum Coefficient (Œ≤)) = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2wHwhiHpCwf"
      },
      "outputs": [],
      "source": [
        "def gradient(x, y):\n",
        "  grad_x=2*x\n",
        "  grad_y=6*y\n",
        "  return # Code Here (Gradients of f(x, y))\n",
        "\n",
        "x, y =2,2\n",
        "learning_rate =0.1\n",
        "momentum =0.9\n",
        "vx, vy = 0.0, 0.0 # initialized velocity\n",
        "\n",
        "for i in range(5):\n",
        "  dx, dy = gradient(x, y)\n",
        "  vx = # Code Here\n",
        "  vy = # Code Here\n",
        "  x += vx\n",
        "  y += vy\n",
        "  print(f\"Step {i+1}: x = {x:.4f}, y = {y:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration |     x       |     y       |   f(x, y)\n",
            "------------------------------------------------\n",
            "        1 | 1.600000 | 0.800000 | 4.480000\n",
            "        2 | 0.920000 | -0.760000 | 2.579200\n",
            "        3 | 0.124000 | -1.708000 | 8.767168\n",
            "        4 | -0.617200 | -1.536400 | 7.462511\n",
            "        5 | -1.160840 | -0.460120 | 1.982681\n"
          ]
        }
      ],
      "source": [
        "# Initialize parameters\n",
        "x, y = 2, 2  # Initial point\n",
        "learning_rate = 0.1  # Learning rate\n",
        "beta = 0.9  # Momentum coefficient\n",
        "iterations = 5  # Number of iterations\n",
        "\n",
        "# Initialize velocities\n",
        "v_x, v_y = 0, 0  # Initial velocities\n",
        "\n",
        "print(\"Iteration |     x       |     y       |   f(x, y)\")\n",
        "print(\"------------------------------------------------\")\n",
        "\n",
        "# Gradient descent with momentum loop\n",
        "for i in range(1, iterations + 1):\n",
        "    # Compute gradients\n",
        "    grad_x = 2 * x\n",
        "    grad_y = 6 * y\n",
        "    \n",
        "    # Update velocities\n",
        "    v_x = beta * v_x + learning_rate * grad_x\n",
        "    v_y = beta * v_y + learning_rate * grad_y\n",
        "    \n",
        "    # Update parameters\n",
        "    x = x - v_x\n",
        "    y = y - v_y\n",
        "    \n",
        "    # Compute the function value\n",
        "    f_xy = x**2 + 3 * y**2\n",
        "    \n",
        "    # Print the iteration details\n",
        "    print(f\"{i:9} | {x:.6f} | {y:.6f} | {f_xy:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2tQRlsUqLvx"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJjaxm6kqNKE"
      },
      "source": [
        "Problem 3: **RMS Prop for AtliQ's Dynamic Pricing Optimization**\n",
        "\n",
        "AtliQ's AI model adjusts product prices dynamically. Implement the RMSProp optimizer for minimizing the function:\n",
        "\n",
        "$$f(w) = w^2 + 5$$\n",
        "\n",
        "Use:\n",
        "\n",
        "* Initial weight (ùë§) = 5.0\n",
        "* Learning rate (Œ∑) = 0.01\n",
        "* Momentum Coefficient(Œ≤)=0.9\n",
        "\n",
        "\n",
        "Run the optimization for 15 iterations and print the weight updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GjcPpNKrwzm"
      },
      "outputs": [],
      "source": [
        "def gradient(w):\n",
        "  grad=2*w\n",
        "  return # Code Here (Gradients of f(w))\n",
        "\n",
        "w =5.0\n",
        "learning_rate =0.01\n",
        "beta =0.9\n",
        "epsilon = 1e-8\n",
        "squared_gradient_average = 0.0 # initialized squared gradient average\n",
        "\n",
        "for i in range(15):\n",
        "  grad = gradient(w)\n",
        "  squared_gradient_average = # Code Here\n",
        "  w = w-beta*grad**2\n",
        "  print(f\"Step {i+1}: w = {w:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0tb736Ysolc"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87npAc9ysqOU"
      },
      "source": [
        "Problem 4: **Adam Optimizer for AtliQ AI Models**\n",
        "\n",
        "AtliQ is training an AI model to recommend warehouse restocking schedules. Use the Adam optimizer to minimize the function:\n",
        "\n",
        "$$f(x) = x^4 - 3x^3 + 2$$\n",
        "\n",
        "Write code to:\n",
        "\n",
        "* Initialize x = 3.0\n",
        "\n",
        "Run the optimizations for 19 iterations (starting from 1) with:\n",
        "* Learning rate (Œ∑) = 0.01\n",
        "* Momentum Coefficients: Œ≤1 = 0.9, Œ≤2 = 0.09\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQjK7sR4twzm"
      },
      "outputs": [],
      "source": [
        "def gradient(x):\n",
        "  return # Code Here (Gradients of f(x))\n",
        "\n",
        "x =\n",
        "learning_rate =\n",
        "beta1, beta2 =\n",
        "epsilon =\n",
        "first_moment, second_moment = 0.0, 0.0 # initialized first and second moment\n",
        "\n",
        "for t in range(1, 20):\n",
        "  grad = gradient(x)\n",
        "  m = # Code here (update biased first moment)\n",
        "  v = # Code here (update biased second moment)\n",
        "  m_hat = # Code here (corrected first moment)\n",
        "  v_hat = # Code here (corrected second moment)\n",
        "  x = # Code here (update rule)\n",
        "  first_moment, second_moment = m, v\n",
        "  print(f\"Step {t}: x = {x:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "953S3HD64qNm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv_cb_dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
